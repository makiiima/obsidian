{"metadata": {"kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "Mindspore-python3.7-aarch64", "language": "python"}, "language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "toc-showcode": true}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "## 1. \u6570\u636e\u540c\u6b65", "metadata": {}}, {"cell_type": "code", "source": "import moxing as mox\n# \u8bf7\u66ff\u6362\u6210\u81ea\u5df1\u7684obs\u8def\u5f84\nmox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/text_classification_mindspore/data/\", dst_url='./data/') ", "metadata": {"trusted": true}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "INFO:root:Using MoXing-v1.17.3-d858ff4a\nINFO:root:Using OBS-Python-SDK-3.20.9.1\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## 2. \u5bfc\u5165\u4f9d\u8d56\u5e93", "metadata": {}}, {"cell_type": "code", "source": "import math\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\nimport random\nimport codecs\nfrom pathlib import Path\n\nimport mindspore\nimport mindspore.dataset as ds\nimport mindspore.nn as nn\nfrom mindspore import Tensor\nfrom mindspore import context\nfrom mindspore.train.model import Model\nfrom mindspore.nn.metrics import Accuracy\nfrom mindspore.train.serialization import load_checkpoint, load_param_into_net\nfrom mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\nfrom mindspore.ops import operations as ops", "metadata": {"trusted": true}, "execution_count": 2, "outputs": [{"name": "stderr", "text": "[WARNING] ME(1756:281472976124208,MainProcess):2021-04-02-02:36:48.894.01 [mindspore/_check_version.py:207] MindSpore version 1.1.1 and \"te\" wheel package version 1.0 does not match, reference to the match info on: https://www.mindspore.cn/install\nMindSpore version 1.1.1 and \"topi\" wheel package version 0.6.0 does not match, reference to the match info on: https://www.mindspore.cn/install\n[WARNING] ME(1756:281472976124208,MainProcess):2021-04-02-02:36:48.757.068 [mindspore/ops/operations/array_ops.py:2302] WARN_DEPRECATED: The usage of Pack is deprecated. Please use Stack.\n", "output_type": "stream"}, {"name": "stdout", "text": "WARNING: 'ControlDepend' is deprecated from version 1.1 and will be removed in a future version, use 'Depend' instead.\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## 3. \u8d85\u53c2\u6570\u8bbe\u7f6e", "metadata": {}}, {"cell_type": "code", "source": "from easydict import EasyDict as edict\n\ncfg = edict({\n    'name': 'movie review',\n    'pre_trained': False,\n    'num_classes': 2,\n    'batch_size': 64,\n    'epoch_size': 4,\n    'weight_decay': 3e-5,\n    'data_path': './data/',\n    'device_target': 'Ascend',\n    'device_id': 0,\n    'keep_checkpoint_max': 1,\n    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n    'word_len': 51,\n    'vec_length': 40\n})", "metadata": {"trusted": true}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)", "metadata": {"trusted": true}, "execution_count": 4, "outputs": []}, {"cell_type": "markdown", "source": "## 4. \u6570\u636e\u9884\u5904\u7406", "metadata": {}}, {"cell_type": "code", "source": "# \u6570\u636e\u9884\u89c8\nwith open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n        print(\"Negative reivews:\")\n        for i in range(5):\n            print(\"[{0}]:{1}\".format(i,f.readline()))\nwith open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n        print(\"Positive reivews:\")\n        for i in range(5):\n            print(\"[{0}]:{1}\".format(i,f.readline()))", "metadata": {"trusted": true}, "execution_count": 5, "outputs": [{"name": "stdout", "text": "Negative reivews:\n[0]:simplistic , silly and tedious . \n\n[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n\n[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n\n[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n\n[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n\nPositive reivews:\n[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n\n[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n\n[2]:effective but too-tepid biopic\n\n[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n\n[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "class Generator():\n    def __init__(self, input_list):\n        self.input_list=input_list\n    def __getitem__(self,item):\n        return (np.array(self.input_list[item][0],dtype=np.int32),\n                np.array(self.input_list[item][1],dtype=np.int32))\n    def __len__(self):\n        return len(self.input_list)\n\n\nclass MovieReview:\n    '''\n    \u5f71\u8bc4\u6570\u636e\u96c6\n    '''\n    def __init__(self, root_dir, maxlen, split):\n        '''\n        input:\n            root_dir: \u5f71\u8bc4\u6570\u636e\u76ee\u5f55\n            maxlen: \u8bbe\u7f6e\u53e5\u5b50\u6700\u5927\u957f\u5ea6\n            split: \u8bbe\u7f6e\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3/\u8bc4\u4f30\u7684\u6bd4\u4f8b\n        '''\n        self.path = root_dir\n        self.feelMap = {\n            'neg':0,\n            'pos':1\n        }\n        self.files = []\n\n        self.doConvert = False\n        \n        mypath = Path(self.path)\n        if not mypath.exists() or not mypath.is_dir():\n            print(\"please check the root_dir!\")\n            raise ValueError\n\n        # \u5728\u6570\u636e\u76ee\u5f55\u4e2d\u627e\u5230\u6587\u4ef6\n        for root,_,filename in os.walk(self.path):\n            for each in filename:\n                self.files.append(os.path.join(root,each))\n            break\n\n        # \u786e\u8ba4\u662f\u5426\u4e3a\u4e24\u4e2a\u6587\u4ef6.neg\u4e0e.pos\n        if len(self.files) != 2:\n            print(\"There are {} files in the root_dir\".format(len(self.files)))\n            raise ValueError\n\n        # \u8bfb\u53d6\u6570\u636e\n        self.word_num = 0\n        self.maxlen = 0\n        self.minlen = float(\"inf\")\n        self.maxlen = float(\"-inf\")\n        self.Pos = []\n        self.Neg = []\n        for filename in self.files:\n            f = codecs.open(filename, 'r')\n            ff = f.read()\n            file_object = codecs.open(filename, 'w', 'utf-8')\n            file_object.write(ff)\n            self.read_data(filename)\n        self.PosNeg = self.Pos + self.Neg\n\n        self.text2vec(maxlen=maxlen)\n        self.split_dataset(split=split)\n\n    def read_data(self, filePath):\n\n        with open(filePath,'r') as f:\n            \n            for sentence in f.readlines():\n                sentence = sentence.replace('\\n','')\\\n                                    .replace('\"','')\\\n                                    .replace('\\'','')\\\n                                    .replace('.','')\\\n                                    .replace(',','')\\\n                                    .replace('[','')\\\n                                    .replace(']','')\\\n                                    .replace('(','')\\\n                                    .replace(')','')\\\n                                    .replace(':','')\\\n                                    .replace('--','')\\\n                                    .replace('-',' ')\\\n                                    .replace('\\\\','')\\\n                                    .replace('0','')\\\n                                    .replace('1','')\\\n                                    .replace('2','')\\\n                                    .replace('3','')\\\n                                    .replace('4','')\\\n                                    .replace('5','')\\\n                                    .replace('6','')\\\n                                    .replace('7','')\\\n                                    .replace('8','')\\\n                                    .replace('9','')\\\n                                    .replace('`','')\\\n                                    .replace('=','')\\\n                                    .replace('$','')\\\n                                    .replace('/','')\\\n                                    .replace('*','')\\\n                                    .replace(';','')\\\n                                    .replace('<b>','')\\\n                                    .replace('%','')\n                sentence = sentence.split(' ')\n                sentence = list(filter(lambda x: x, sentence))\n                if sentence:\n                    self.word_num += len(sentence)\n                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n                    if 'pos' in filePath:\n                        self.Pos.append([sentence,self.feelMap['pos']])\n                    else:\n                        self.Neg.append([sentence,self.feelMap['neg']])\n\n    def text2vec(self, maxlen):\n        '''\n        \u5c06\u53e5\u5b50\u8f6c\u5316\u4e3a\u5411\u91cf\n\n        '''\n        # Vocab = {word : index}\n        self.Vocab = dict()\n\n        # self.Vocab['None']\n        for SentenceLabel in self.Pos+self.Neg:\n            vector = [0]*maxlen\n            for index, word in enumerate(SentenceLabel[0]):\n                if index >= maxlen:\n                    break\n                if word not in self.Vocab.keys():\n                    self.Vocab[word] = len(self.Vocab)\n                    vector[index] = len(self.Vocab) - 1\n                else:\n                    vector[index] = self.Vocab[word]\n            SentenceLabel[0] = vector\n        self.doConvert = True\n\n    def split_dataset(self, split):\n        '''\n        \u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u4e0e\u6d4b\u8bd5\u96c6\n\n        '''\n\n        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n        trunk_num = int(1/(1-split))\n        pos_temp=list()\n        neg_temp=list()\n        for index in range(trunk_num):\n            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n        self.train = [i for item in pos_temp+neg_temp for i in item]\n\n        random.shuffle(self.train)\n        # random.shuffle(self.test)\n\n    def get_dict_len(self):\n        '''\n        \u83b7\u5f97\u6570\u636e\u96c6\u4e2d\u6587\u5b57\u7ec4\u6210\u7684\u8bcd\u5178\u957f\u5ea6\n        '''\n        if self.doConvert:\n            return len(self.Vocab)\n        else:\n            print(\"Haven't finished Text2Vec\")\n            return -1\n\n    def create_train_dataset(self, epoch_size, batch_size):\n        dataset = ds.GeneratorDataset(\n                                        source=Generator(input_list=self.train), \n                                        column_names=[\"data\",\"label\"], \n                                        shuffle=False\n                                        )\n#         dataset.set_dataset_size(len(self.train))\n        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n        dataset=dataset.repeat(epoch_size)\n        return dataset\n\n    def create_test_dataset(self, batch_size):\n        dataset = ds.GeneratorDataset(\n                                        source=Generator(input_list=self.test), \n                                        column_names=[\"data\",\"label\"], \n                                        shuffle=False\n                                        )\n#         dataset.set_dataset_size(len(self.test))\n        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n        return dataset", "metadata": {"trusted": true}, "execution_count": 6, "outputs": []}, {"cell_type": "code", "source": "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\ndataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\nbatch_num = dataset.get_dataset_size() ", "metadata": {"trusted": true}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": "vocab_size=instance.get_dict_len()\nprint(\"vocab_size:{0}\".format(vocab_size))\nitem =dataset.create_dict_iterator()\nfor i,data in enumerate(item):\n    if i<1:\n        print(data)\n        print(data['data'][1])\n    else:\n        break", "metadata": {"trusted": true}, "execution_count": 8, "outputs": [{"name": "stdout", "text": "vocab_size:18848\n{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n[[   15,  3190,  6781 ...     0,     0,     0],\n [ 1320,   582,     4 ...     0,     0,     0],\n [ 1734,   111,    36 ...     0,     0,     0],\n ...\n [   82,    94,   367 ...     0,     0,     0],\n [10449,    55,  2923 ...     0,     0,     0],\n [  336,   203,   272 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, \n 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, \n 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1])}\n[ 1320   582     4  3070     0   603  5507 12780    32 12781  1304   669\n   896  1310   122     4    82     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0]\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## 5.\u6a21\u578b\u8bad\u7ec3", "metadata": {}}, {"cell_type": "markdown", "source": "### 5.1\u8bad\u7ec3\u53c2\u6570\u8bbe\u7f6e", "metadata": {}}, {"cell_type": "code", "source": "learning_rate = []\nwarm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n           for i in range(math.floor(cfg.epoch_size / 5))]\nshrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\nnormal_run = [1e-3 for _ in range(batch_num) for i in \n              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n                    - math.floor(cfg.epoch_size * 2 / 5))]\nlearning_rate = learning_rate + warm_up + normal_run + shrink", "metadata": {"trusted": true}, "execution_count": 9, "outputs": []}, {"cell_type": "code", "source": "def _weight_variable(shape, factor=0.01):\n    init_value = np.random.randn(*shape).astype(np.float32) * factor\n    return Tensor(init_value)\n\n\ndef make_conv_layer(kernel_size):\n    weight_shape = (96, 1, *kernel_size)\n    weight = _weight_variable(weight_shape)\n    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n\n\nclass TextCNN(nn.Cell):\n    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n        super(TextCNN, self).__init__()\n        self.vec_length = vec_length\n        self.word_len = word_len\n        self.num_classes = num_classes\n\n        self.unsqueeze = ops.ExpandDims()\n        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n\n        self.slice = ops.Slice()\n        self.layer1 = self.make_layer(kernel_height=3)\n        self.layer2 = self.make_layer(kernel_height=4)\n        self.layer3 = self.make_layer(kernel_height=5)\n\n        self.concat = ops.Concat(1)\n\n        self.fc = nn.Dense(96*3, self.num_classes)\n        self.drop = nn.Dropout(keep_prob=0.5)\n        self.print = ops.Print()\n        self.reducemean = ops.ReduceMax(keep_dims=False)\n        \n    def make_layer(self, kernel_height):\n        return nn.SequentialCell(\n            [\n                make_conv_layer((kernel_height,self.vec_length)),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n            ]\n        )\n\n    def construct(self,x):\n        x = self.unsqueeze(x, 1)\n        x = self.embedding(x)\n        x1 = self.layer1(x)\n        x2 = self.layer2(x)\n        x3 = self.layer3(x)\n\n        x1 = self.reducemean(x1, (2, 3))\n        x2 = self.reducemean(x2, (2, 3))\n        x3 = self.reducemean(x3, (2, 3))\n\n        x = self.concat((x1, x2, x3))\n        x = self.drop(x)\n        x = self.fc(x)\n        return x", "metadata": {"trusted": true}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n              num_classes=cfg.num_classes, vec_length=cfg.vec_length)", "metadata": {"trusted": true}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": "print(net)", "metadata": {"trusted": true}, "execution_count": 12, "outputs": [{"name": "stdout", "text": "TextCNN<\n  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table), dtype=Float32, padding_idx=None>\n  (layer1): SequentialCell<\n    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40),stride=(1, 1),  pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=Trueweight_init=[[[[-1.01975221e-02 -6.93383277e-04  4.07581311e-03 ... -4.57499130e-03\n         2.05630157e-03 -3.86761921e-03]\n       [ 5.27941715e-03 -5.78200154e-04  1.88886896e-02 ... -1.42225558e-02\n         3.56013328e-03 -1.76590439e-02]\n       [-1.47636037e-03  1.24138523e-05  2.62513896e-03 ...  4.56628995e-03\n        -4.60443925e-03  5.23316732e-04]]]\n    \n    \n     [[[ 1.80724217e-03  5.58177708e-03  9.58025362e-03 ...  8.16981215e-03\n         2.29106238e-03  1.75089308e-03]\n       [ 2.15005642e-03 -1.70884822e-02 -2.57208943e-03 ... -5.47323236e-03\n         2.04329342e-02 -2.61869072e-03]\n       [ 8.53526313e-03 -1.11518183e-03  6.77704671e-03 ... -1.09469565e-02\n        -8.85556859e-04  1.62346419e-02]]]\n    \n    \n     [[[ 1.05571337e-02  1.02827270e-02  7.08943466e-03 ... -7.58388219e-03\n        -6.97487779e-03 -5.68528380e-03]\n       [-1.69466611e-03 -8.01722333e-03 -2.59528006e-03 ...  7.05966214e-03\n         8.20585247e-03  1.51339360e-02]\n       [-1.10727474e-02 -4.44684690e-03  4.66134585e-03 ...  2.85023329e-04\n         1.45670259e-02 -1.30835073e-02]]]\n    \n    \n     ...\n    \n    \n     [[[ 7.29244552e-04 -8.12886516e-04  7.41507672e-03 ... -4.10588272e-03\n         5.37596527e-04 -2.40132911e-03]\n       [-1.19297989e-02  6.60708547e-03  6.75804971e-04 ... -3.80326062e-03\n        -1.17351841e-02 -2.40278011e-03]\n       [ 4.79531894e-03 -5.88043639e-03 -1.48151498e-02 ...  2.10125046e-03\n        -6.45607943e-04 -5.41895395e-03]]]\n    \n    \n     [[[-1.83240825e-03  5.43291261e-03  7.10784039e-03 ... -2.70072818e-02\n         8.53812380e-04  2.29081349e-03]\n       [ 1.84109248e-02  1.83647301e-03  6.15894038e-04 ...  6.07677922e-03\n         1.48664592e-02  3.42242979e-03]\n       [ 2.40820441e-02 -4.23888676e-03  1.56482570e-02 ... -1.89700369e-02\n         4.46266774e-03  2.08784896e-03]]]\n    \n    \n     [[[-5.29482728e-03 -9.43536963e-03 -6.72342116e-03 ... -3.29096220e-03\n        -1.00514204e-04 -1.27961505e-02]\n       [-7.11471424e-04  3.76987882e-04  9.77989007e-03 ... -9.41509008e-03\n         5.21176634e-03 -1.84813328e-02]\n       [-1.04572345e-02 -2.32955371e-03 -6.10041525e-03 ...  9.02488921e-03\n         1.38039589e-02  6.35558600e-03]]]], bias_init=zeros, format=NCHW>\n    (1): ReLU<>\n    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n    >\n  (layer2): SequentialCell<\n    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40),stride=(1, 1),  pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=Trueweight_init=[[[[ 3.4271076e-03  8.9845415e-03  1.7176390e-02 ... -2.9000149e-03\n         1.0413056e-02  1.8878920e-02]\n       [ 1.2836415e-02  2.7134572e-04 -3.2851091e-03 ...  1.3853388e-03\n         1.5680760e-02 -1.5339485e-02]\n       [ 5.2072885e-03 -2.7773778e-03 -7.9283314e-03 ... -7.0501119e-03\n        -6.2660412e-03  5.3412626e-03]\n       [ 9.1624185e-03  6.1636638e-05  1.1323442e-02 ... -1.8328846e-03\n         1.0890803e-02  8.1075635e-03]]]\n    \n    \n     [[[ 2.1277487e-02  7.7600144e-03 -1.6961701e-02 ...  1.2806475e-03\n         3.5466203e-03  4.2962562e-03]\n       [-7.0943398e-04  9.7264542e-04  5.6935935e-03 ...  2.1027619e-02\n         1.1466548e-02  5.8571142e-03]\n       [-5.5385237e-03  1.4731588e-03 -1.8708872e-02 ...  6.8696742e-03\n         1.1364183e-02  2.8502978e-02]\n       [-2.4210294e-03  6.9721495e-03  6.7677801e-03 ... -1.5072146e-02\n         3.7238661e-03 -1.7722677e-02]]]\n    \n    \n     [[[ 2.0841356e-02 -5.5947839e-03  1.5548697e-03 ... -2.8236578e-03\n         2.8949368e-03 -4.3771490e-03]\n       [ 2.2186600e-03 -1.1632005e-02  9.4650529e-04 ... -2.7542054e-03\n         3.2039597e-03 -2.5674473e-03]\n       [ 7.5164917e-03 -7.9974737e-03  6.6294358e-03 ... -2.0571537e-02\n        -6.1667776e-03 -7.7378131e-03]\n       [ 1.3070641e-02  1.2145782e-03  6.9544637e-03 ... -8.3221644e-03\n         7.6622861e-03  2.3599975e-04]]]\n    \n    \n     ...\n    \n    \n     [[[ 1.2157369e-02  2.2094753e-02  8.2681170e-03 ...  7.0307413e-03\n         1.7739513e-03 -5.7638143e-03]\n       [-1.0695128e-02 -1.1607397e-03  7.7116708e-03 ...  7.9374947e-03\n        -4.1872882e-03 -4.9673435e-03]\n       [-2.8225856e-03  6.0150847e-03  2.9882353e-03 ... -3.7366797e-03\n         6.7756353e-03 -1.4647303e-02]\n       [-1.8517365e-03  2.9673185e-03 -7.3632919e-03 ...  7.0564396e-04\n        -1.3410307e-02 -4.0863943e-03]]]\n    \n    \n     [[[-1.2848447e-03 -3.0739822e-03  1.6846674e-02 ... -9.1170659e-03\n        -9.0347296e-03 -1.9096570e-02]\n       [-1.0032056e-02  2.3219537e-03 -4.1512656e-03 ... -8.9047372e-04\n         2.8588218e-04  1.7372811e-02]\n       [ 8.7444866e-03  3.8105189e-03  5.0835032e-03 ...  3.9129863e-03\n         1.0350514e-02  1.7767347e-02]\n       [-1.3506732e-02 -1.4255235e-02 -1.5193651e-02 ...  3.9721983e-03\n         9.0690823e-03 -2.4886047e-02]]]\n    \n    \n     [[[ 9.2416350e-03  1.4977989e-02  2.1778185e-02 ...  3.9517372e-03\n        -8.2300534e-04  5.9668249e-03]\n       [-5.4674363e-03  2.9111996e-03 -1.6778689e-02 ...  1.0264850e-03\n         7.9059467e-04 -8.4615238e-03]\n       [-4.1162083e-03 -2.4206988e-03  8.8477591e-03 ...  1.7782494e-03\n         4.5224330e-03 -4.7719896e-05]\n       [-1.4600134e-02 -6.6143721e-03 -9.8061254e-03 ...  9.7008441e-03\n        -6.5608607e-03  1.3068032e-04]]]], bias_init=zeros, format=NCHW>\n    (1): ReLU<>\n    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n    >\n  (layer3): SequentialCell<\n    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40),stride=(1, 1),  pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=Trueweight_init=[[[[-2.21139286e-03 -2.41488521e-03  2.13555936e-02 ... -1.65794473e-02\n        -7.10360997e-04 -1.62575231e-03]\n       [ 1.11704292e-02 -1.78829674e-02 -1.14661106e-03 ... -1.14609059e-02\n        -3.68877198e-03 -4.50790540e-04]\n       [-7.39941327e-03 -4.23143414e-04  1.97958127e-02 ... -9.11398139e-03\n         8.92375072e-04  1.19612953e-02]\n       [-4.74007474e-03  1.00720944e-02  3.63600045e-03 ...  1.67858973e-02\n        -2.58720089e-02  8.07726383e-03]\n       [-7.80701917e-03 -7.49023072e-03  2.31460156e-03 ... -5.95709542e-03\n         1.25484234e-02 -7.18322815e-03]]]\n    \n    \n     [[[ 4.05057101e-03  6.22314075e-03 -1.79613626e-03 ... -1.04579255e-02\n         6.66824076e-03 -5.07642375e-03]\n       [-6.33907551e-03 -5.24501549e-03  9.81074572e-03 ...  1.08612441e-02\n         1.70226244e-03  7.94867240e-03]\n       [ 4.94264718e-03 -1.35119352e-02 -7.92734139e-03 ... -1.54358121e-02\n         7.29021558e-04  2.12917291e-02]\n       [-1.20290322e-02  1.84835494e-03 -1.47105183e-03 ...  1.58773959e-02\n         5.38740726e-03  1.24286087e-02]\n       [-7.55051966e-04 -3.56710283e-03  7.49293063e-03 ...  1.35142896e-02\n        -8.74322373e-03  3.62198614e-03]]]\n    \n    \n     [[[ 1.23833949e-02 -6.38807379e-03 -2.67221536e-02 ... -9.67524713e-04\n        -1.45942960e-02 -9.44472733e-04]\n       [-9.08638909e-03 -9.78543237e-03 -3.33389640e-03 ... -3.80483689e-03\n         1.16851386e-02  1.43156922e-03]\n       [ 1.43256458e-02 -5.70932869e-03 -1.13590928e-02 ... -1.99383404e-02\n        -6.43174350e-03  1.42686237e-02]\n       [-1.46491197e-03 -5.53586870e-05 -3.01511330e-03 ... -1.41065866e-02\n        -2.57367850e-03 -6.79661753e-03]\n       [-4.87489766e-03  7.77473859e-03 -1.12401610e-02 ...  4.49080952e-03\n        -4.50980524e-03 -8.48950935e-04]]]\n    \n    \n     ...\n    \n    \n     [[[ 7.76800327e-03 -9.63745266e-03  7.49069266e-03 ... -1.61703618e-03\n        -1.16275484e-03  2.07886496e-03]\n       [ 1.02279056e-03  1.56720411e-02  9.58159752e-03 ... -7.44965533e-03\n         4.54464648e-03 -7.19113462e-03]\n       [ 1.53354798e-02  4.20627730e-05 -1.24453008e-03 ... -3.67996749e-03\n         3.53310304e-03 -1.08989011e-02]\n       [ 3.17901955e-03 -4.66305204e-03  6.87431311e-03 ... -1.95163600e-02\n         3.92241823e-03  1.11916359e-03]\n       [ 2.58645299e-03 -1.48356836e-02  1.63387097e-02 ... -8.05650780e-04\n        -1.30316457e-02 -3.01466876e-04]]]\n    \n    \n     [[[ 3.19446251e-03  2.96817850e-02 -1.40278563e-02 ... -1.84968859e-02\n         2.09035864e-03  2.98187509e-03]\n       [-1.55713316e-02 -7.12589920e-03 -8.18836503e-03 ... -8.60458426e-03\n        -9.79105849e-03 -1.56782549e-02]\n       [-7.78470794e-03  9.11665242e-03  1.06965341e-02 ...  1.00400383e-02\n         2.14654971e-02 -4.33663465e-03]\n       [-3.12025397e-04 -1.52020631e-02 -1.80257801e-02 ... -5.49256569e-03\n        -9.82420333e-03  7.44856708e-03]\n       [ 2.09764205e-02  3.55819124e-03 -1.49949372e-03 ...  8.69765878e-03\n        -2.15322757e-03 -9.84702073e-03]]]\n    \n    \n     [[[ 1.28701515e-02  1.09110577e-02  1.85842607e-02 ...  7.26271654e-03\n        -8.93867668e-03  5.60572185e-03]\n       [-1.93308294e-02 -1.46030646e-03 -6.15600077e-03 ...  1.00303730e-02\n        -7.92537536e-03  1.08167678e-02]\n       [ 1.47010405e-02 -1.63789820e-02 -8.38982966e-03 ...  7.99456704e-03\n        -4.81590442e-03  3.47689202e-04]\n       [-5.46710053e-03 -1.18273115e-02 -4.41340409e-04 ... -2.90321792e-03\n         4.37540584e-04 -3.17701045e-03]\n       [-2.04848833e-02  4.65657981e-03  4.18039411e-03 ... -4.22161072e-04\n        -1.47479621e-03 -2.24956870e-03]]]], bias_init=zeros, format=NCHW>\n    (1): ReLU<>\n    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n    >\n  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n  (drop): Dropout<keep_prob=0.5>\n  >\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# Continue training if set pre_trained to be True\nif cfg.pre_trained:\n    param_dict = load_checkpoint(cfg.checkpoint_path)\n    load_param_into_net(net, param_dict)", "metadata": {"trusted": true}, "execution_count": 13, "outputs": []}, {"cell_type": "code", "source": "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\nloss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)", "metadata": {"trusted": true}, "execution_count": 14, "outputs": []}, {"cell_type": "code", "source": "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})", "metadata": {"trusted": true}, "execution_count": 15, "outputs": []}, {"cell_type": "code", "source": "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\ntime_cb = TimeMonitor(data_size=batch_num)\nckpt_save_dir = \"./ckpt\"\nckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\nloss_cb = LossMonitor()", "metadata": {"trusted": true}, "execution_count": 16, "outputs": []}, {"cell_type": "code", "source": "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\nprint(\"train success\")", "metadata": {"trusted": true}, "execution_count": 17, "outputs": [{"name": "stdout", "text": "epoch: 1 step: 596, loss is 0.07209684\nepoch time: 39359.259 ms, per step time: 66.039 ms\nepoch: 2 step: 596, loss is 0.0029934864\nepoch time: 4308.688 ms, per step time: 7.229 ms\nepoch: 3 step: 596, loss is 0.0019718197\nepoch time: 4266.735 ms, per step time: 7.159 ms\nepoch: 4 step: 596, loss is 0.0011571363\nepoch time: 4309.405 ms, per step time: 7.231 ms\ntrain success\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## 6. \u6d4b\u8bd5\u8bc4\u4f30", "metadata": {}}, {"cell_type": "code", "source": "checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'", "metadata": {"trusted": true}, "execution_count": 18, "outputs": []}, {"cell_type": "code", "source": "dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\nopt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n              learning_rate=0.001, weight_decay=cfg.weight_decay)\nloss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\nnet = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n\nif checkpoint_path is not None:\n    param_dict = load_checkpoint(checkpoint_path)\n    print(\"load checkpoint from [{}].\".format(checkpoint_path))\nelse:\n    param_dict = load_checkpoint(cfg.checkpoint_path)\n    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n\nload_param_into_net(net, param_dict)\nnet.set_train(False)\nmodel = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n\nacc = model.eval(dataset)\nprint(\"accuracy: \", acc)", "metadata": {"trusted": true}, "execution_count": 19, "outputs": [{"name": "stdout", "text": "load checkpoint from [./ckpt/train_textcnn-4_596.ckpt].\naccuracy:  {'acc': 0.763671875}\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## 7. \u5728\u7ebf\u6d4b\u8bd5", "metadata": {}}, {"cell_type": "code", "source": "def preprocess(sentence):\n    sentence = sentence.lower().strip()\n    sentence = sentence.replace('\\n','')\\\n                                    .replace('\"','')\\\n                                    .replace('\\'','')\\\n                                    .replace('.','')\\\n                                    .replace(',','')\\\n                                    .replace('[','')\\\n                                    .replace(']','')\\\n                                    .replace('(','')\\\n                                    .replace(')','')\\\n                                    .replace(':','')\\\n                                    .replace('--','')\\\n                                    .replace('-',' ')\\\n                                    .replace('\\\\','')\\\n                                    .replace('0','')\\\n                                    .replace('1','')\\\n                                    .replace('2','')\\\n                                    .replace('3','')\\\n                                    .replace('4','')\\\n                                    .replace('5','')\\\n                                    .replace('6','')\\\n                                    .replace('7','')\\\n                                    .replace('8','')\\\n                                    .replace('9','')\\\n                                    .replace('`','')\\\n                                    .replace('=','')\\\n                                    .replace('$','')\\\n                                    .replace('/','')\\\n                                    .replace('*','')\\\n                                    .replace(';','')\\\n                                    .replace('<b>','')\\\n                                    .replace('%','')\\\n                                    .replace(\"  \",\" \")\n    sentence = sentence.split(' ')\n    maxlen = cfg.word_len\n    vector = [0]*maxlen\n    for index, word in enumerate(sentence):\n        if index >= maxlen:\n            break\n        if word not in instance.Vocab.keys():\n            print(word,\"\u5355\u8bcd\u672a\u51fa\u73b0\u5728\u5b57\u5178\u4e2d\")\n        else:\n            vector[index] = instance.Vocab[word]\n    sentence = vector\n\n    return sentence\n\ndef inference(review_en):\n    review_en = preprocess(review_en)\n    input_en = Tensor(np.array([review_en]).astype(np.int32))\n    output = net(input_en)\n    if np.argmax(np.array(output[0])) == 1:\n        print(\"Positive comments\")\n    else:\n        print(\"Negative comments\")", "metadata": {"trusted": true}, "execution_count": 20, "outputs": []}, {"cell_type": "code", "source": "review_en = \"the movie is so boring\"\ninference(review_en)", "metadata": {"trusted": true}, "execution_count": 27, "outputs": [{"name": "stdout", "text": "Negative comments\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}