{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 1.导入模块"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[WARNING] ME(879:281472949586224,MainProcess):2021-03-29-00:56:58.698.70 [mindspore/_check_version.py:207] MindSpore version 1.1.1 and \"te\" wheel package version 1.0 does not match, reference to the match info on: https://www.mindspore.cn/install\n","MindSpore version 1.1.1 and \"topi\" wheel package version 0.6.0 does not match, reference to the match info on: https://www.mindspore.cn/install\n","[WARNING] ME(879:281472949586224,MainProcess):2021-03-29-00:56:58.864.357 [mindspore/ops/operations/array_ops.py:2302] WARN_DEPRECATED: The usage of Pack is deprecated. Please use Stack.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING: 'ControlDepend' is deprecated from version 1.1 and will be removed in a future version, use 'Depend' instead.\n"]}],"source":["import os\n","import numpy as np\n","import re\n","import sys\n","import random\n","import unicodedata\n","import math\n","\n","from mindspore import Tensor, nn, Model, context\n","from mindspore.train.serialization import load_param_into_net, load_checkpoint\n","from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\n","from mindspore import dataset as ds\n","from mindspore.mindrecord import FileWriter\n","from mindspore import Parameter\n","from mindspore.nn.loss.loss import _Loss\n","from mindspore.ops import functional as F\n","from mindspore.ops import operations as P\n","from mindspore.common import dtype as mstype"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target='Ascend')"]},{"cell_type":"markdown","metadata":{},"source":["## 2.数据预处理"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1数据预处理细节"]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Hi.\t嗨。\n","\n","Hi.\t你好。\n","\n","Run.\t你用跑的。\n","\n","Wait!\t等等！\n","\n","Hello!\t你好。\n","\n","I try.\t让我来。\n","\n","I won!\t我赢了。\n","\n","Oh no!\t不会吧。\n","\n","Cheers!\t干杯!\n","\n","He ran.\t他跑了。\n","\n"]}],"source":["#查看训练数据内容前10行内容\n","with open(\"./data/cmn_zhsim.txt\", 'r', encoding='utf-8') as f:\n","        for i in range(10):\n","            print(f.readline())"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["EOS = \"<eos>\"\n","SOS = \"<sos>\"\n","MAX_SEQ_LEN=10"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["#我们需要将字符转化为ASCII编码\n","#并全部转化为小写字母，并修剪大部分标点符号\n","#除了(a-z, A-Z, \".\", \"?\", \"!\", \",\")这些字符外，全替换成空格\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","def normalizeString(s):\n","    s = s.lower().strip()\n","    s = unicodeToAscii(s)\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s"]},{"cell_type":"code","execution_count":65,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_data(data_path, vocab_save_path, max_seq_len):\n","    with open(data_path, 'r', encoding='utf-8') as f:\n","        data = f.read()\n","\n","    # 读取文本文件，按行分割，再将每行分割成语句对\n","    data = data.split('\\n')\n","\n","     # 截取前2000行数据进行训练\n","    data = data[:2000]\n","\n","    # 分割每行中的中英文\n","    en_data = [normalizeString(line.split('\\t')[0]) for line in data]\n","\n","    ch_data = [line.split('\\t')[1] for line in data]\n","\n","    # 利用集合，获得中英文词汇表\n","    en_vocab = set(' '.join(en_data).split(' '))\n","    id2en = [EOS] + [SOS] + list(en_vocab)\n","    en2id = {c:i for i,c in enumerate(id2en)}\n","    en_vocab_size = len(id2en)\n","    # np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n","\n","    ch_vocab = set(''.join(ch_data))\n","    id2ch = [EOS] + [SOS] + list(ch_vocab)\n","    ch2id = {c:i for i,c in enumerate(id2ch)}\n","    ch_vocab_size = len(id2ch)\n","    # np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n","\n","    # 将句子用词汇表id表示\n","    en_num_data = np.array([[1] + [int(en2id[en]) for en in line.split(' ')] + [0] for line in en_data])\n","    ch_num_data = np.array([[1] + [int(ch2id[ch]) for ch in line] + [0] for line in ch_data])\n","\n","    #将短句子扩充到统一的长度\n","    for i in range(len(en_num_data)):\n","        num = max_seq_len + 1 - len(en_num_data[i])\n","        if(num >= 0):\n","            en_num_data[i] += [0]*num\n","        else:\n","            en_num_data[i] = en_num_data[i][:max_seq_len] + [0]\n","\n","    for i in range(len(ch_num_data)):\n","        num = max_seq_len + 1 - len(ch_num_data[i])\n","        if(num >= 0):\n","            ch_num_data[i] += [0]*num\n","        else:\n","            ch_num_data[i] = ch_num_data[i][:max_seq_len] + [0]\n","    \n","    \n","    np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n","    \n","    np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n","\n","    return en_num_data, ch_num_data, en_vocab_size, ch_vocab_size"]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[],"source":["#将处理后的数据保存为mindrecord文件，方便后续训练\n","def convert_to_mindrecord(data_path, mindrecord_save_path, max_seq_len):\n","    en_num_data, ch_num_data, en_vocab_size, ch_vocab_size = prepare_data(data_path, mindrecord_save_path, max_seq_len)\n","\n","    # 输出前十行英文句子对应的数据\n","    for i in range(10):\n","        print(en_num_data[i])\n","    \n","    data_list_train = []\n","    for en, ch in zip(en_num_data, ch_num_data):\n","        en = np.array(en).astype(np.int32)\n","        ch = np.array(ch).astype(np.int32)\n","        data_json = {\"encoder_data\": en.reshape(-1),\n","                     \"decoder_data\": ch.reshape(-1)}\n","        data_list_train.append(data_json)\n","    \n","    data_list_eval = random.sample(data_list_train, 20)\n","\n","    data_dir = os.path.join(mindrecord_save_path, \"gru_train.mindrecord\")\n","    writer = FileWriter(data_dir)\n","    schema_json = {\"encoder_data\": {\"type\": \"int32\", \"shape\": [-1]},\n","                   \"decoder_data\": {\"type\": \"int32\", \"shape\": [-1]}}\n","    writer.add_schema(schema_json, \"gru_schema\")\n","    writer.write_raw_data(data_list_train)\n","    writer.commit()\n","\n","    data_dir = os.path.join(mindrecord_save_path, \"gru_eval.mindrecord\")\n","    writer = FileWriter(data_dir)\n","    writer.add_schema(schema_json, \"gru_schema\")\n","    writer.write_raw_data(data_list_eval)\n","    writer.commit()\n","\n","    print(\"en_vocab_size: \", en_vocab_size)\n","    print(\"ch_vocab_size: \", ch_vocab_size)\n","\n","    return en_vocab_size, ch_vocab_size"]},{"cell_type":"code","execution_count":69,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 624, 1061, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 624, 1061, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 288, 1061, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 819, 893, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 969, 893, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 427, 119, 1061, 0, 0, 0, 0, 0, 0, 0]\n","[1, 427, 851, 893, 0, 0, 0, 0, 0, 0, 0]\n","[1, 169, 101, 893, 0, 0, 0, 0, 0, 0, 0]\n","[1, 378, 893, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 859, 446, 1061, 0, 0, 0, 0, 0, 0, 0]\n","en_vocab_size:  1154\n","ch_vocab_size:  1116\n"]},{"data":{"text/plain":["(1154, 1116)"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["if not os.path.exists(\"./preprocess\"):\n","    os.mkdir('./preprocess')\n","convert_to_mindrecord(\"./data/cmn_zhsim.txt\", './preprocess', MAX_SEQ_LEN)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.训练过程"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 超参数设置"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["from easydict import EasyDict as edict\n","\n","# CONFIG\n","cfg = edict({\n","    'en_vocab_size': 1154,\n","    'ch_vocab_size': 1116,\n","    'max_seq_length': 10,\n","    'hidden_size': 1024,\n","    'batch_size': 16,\n","    'eval_batch_size': 1,\n","    'learning_rate': 0.001,\n","    'momentum': 0.9,\n","    'num_epochs': 15,\n","    'save_checkpoint_steps': 125,\n","    'keep_checkpoint_max': 10,\n","    'dataset_path':'./preprocess',\n","    'ckpt_save_path':'./ckpt',\n","    'checkpoint_path':'./ckpt/gru-15_125.ckpt'\n","})"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 读取数据"]},{"cell_type":"code","execution_count":71,"metadata":{"trusted":true},"outputs":[],"source":["def target_operation(encoder_data, decoder_data):\n","    encoder_data = encoder_data[1:]\n","    target_data = decoder_data[1:]\n","    decoder_data = decoder_data[:-1]\n","    return encoder_data, decoder_data, target_data\n","\n","def eval_operation(encoder_data, decoder_data):\n","    encoder_data = encoder_data[1:]\n","    decoder_data = decoder_data[:-1]\n","    return encoder_data, decoder_data\n","\n","def create_dataset(data_home, batch_size, repeat_num=1, is_training=True, device_num=1, rank=0):\n","    if is_training:\n","        data_dir = os.path.join(data_home, \"gru_train.mindrecord\")\n","    else:\n","        data_dir = os.path.join(data_home, \"gru_eval.mindrecord\")\n","    data_set = ds.MindDataset(data_dir, columns_list=[\"encoder_data\",\"decoder_data\"], \n","                              num_parallel_workers=4,\n","                              num_shards=device_num, shard_id=rank)\n","    if is_training:\n","        operations = target_operation\n","        data_set = data_set.map(operations=operations, \n","                                input_columns=[\"encoder_data\",\"decoder_data\"],\n","                    output_columns=[\"encoder_data\",\"decoder_data\",\"target_data\"],\n","                    column_order=[\"encoder_data\",\"decoder_data\",\"target_data\"])\n","    else:\n","        operations = eval_operation\n","        data_set = data_set.map(operations=operations, \n","                                input_columns=[\"encoder_data\",\"decoder_data\"],\n","                   output_columns=[\"encoder_data\",\"decoder_data\"],\n","                   column_order=[\"encoder_data\",\"decoder_data\"])\n","    data_set = data_set.shuffle(buffer_size=data_set.get_dataset_size())\n","    data_set = data_set.batch(batch_size=batch_size, drop_remainder=True)\n","    data_set = data_set.repeat(count=repeat_num)\n","    return data_set"]},{"cell_type":"code","execution_count":72,"metadata":{"trusted":true},"outputs":[],"source":["ds_train = create_dataset(cfg.dataset_path, cfg.batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 网络模型"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["def gru_default_state(batch_size, input_size, hidden_size, num_layers=1, bidirectional=False):\n","    '''Weight init for gru cell'''\n","    stdv = 1 / math.sqrt(hidden_size)\n","    weight_i = Parameter(Tensor(\n","        np.random.uniform(-stdv, stdv, (input_size, 3*hidden_size)).astype(np.float32)), \n","                         name='weight_i')\n","    weight_h = Parameter(Tensor(\n","        np.random.uniform(-stdv, stdv, (hidden_size, 3*hidden_size)).astype(np.float32)), \n","                         name='weight_h')\n","    bias_i = Parameter(Tensor(\n","        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_i')\n","    bias_h = Parameter(Tensor(\n","        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_h')\n","    return weight_i, weight_h, bias_i, bias_h\n","\n","class GRU(nn.Cell):\n","    def __init__(self, config, is_training=True):\n","        super(GRU, self).__init__()\n","        if is_training:\n","            self.batch_size = config.batch_size\n","        else:\n","            self.batch_size = config.eval_batch_size\n","        self.hidden_size = config.hidden_size\n","        self.weight_i, self.weight_h, self.bias_i, self.bias_h = \\\n","            gru_default_state(self.batch_size, self.hidden_size, self.hidden_size)\n","        self.rnn = P.DynamicGRUV2()\n","        self.cast = P.Cast()\n","\n","    def construct(self, x, hidden):\n","        x = self.cast(x, mstype.float16)\n","        y1, h1, _, _, _, _ = self.rnn(x, self.weight_i, self.weight_h, self.bias_i, self.bias_h, None, hidden)\n","        return y1, h1\n","\n","\n","class Encoder(nn.Cell):\n","    def __init__(self, config, is_training=True):\n","        super(Encoder, self).__init__()\n","        self.vocab_size = config.en_vocab_size\n","        self.hidden_size = config.hidden_size\n","        if is_training:\n","            self.batch_size = config.batch_size\n","        else:\n","            self.batch_size = config.eval_batch_size\n","\n","        self.trans = P.Transpose()\n","        self.perm = (1, 0, 2)\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n","        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n","\n","    def construct(self, encoder_input):\n","        embeddings = self.embedding(encoder_input)\n","        embeddings = self.trans(embeddings, self.perm)\n","        output, hidden = self.gru(embeddings, self.h)\n","        return output, hidden\n","\n","class Decoder(nn.Cell):\n","    def __init__(self, config, is_training=True, dropout=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.vocab_size = config.ch_vocab_size\n","        self.hidden_size = config.hidden_size\n","        self.max_len = config.max_seq_length\n","\n","        self.trans = P.Transpose()\n","        self.perm = (1, 0, 2)\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.dropout = nn.Dropout(1-dropout)\n","        self.attn = nn.Dense(self.hidden_size, self.max_len)\n","        self.softmax = nn.Softmax(axis=2)\n","        self.bmm = P.BatchMatMul()\n","        self.concat = P.Concat(axis=2)\n","        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n","\n","        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n","        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n","        self.logsoftmax = nn.LogSoftmax(axis=2)\n","        self.cast = P.Cast()\n","\n","    def construct(self, decoder_input, hidden, encoder_output):\n","        embeddings = self.embedding(decoder_input)\n","        embeddings = self.dropout(embeddings)\n","        # calculate attn\n","        attn_weights = self.softmax(self.attn(embeddings))\n","        encoder_output = self.trans(encoder_output, self.perm)\n","        attn_applied = self.bmm(attn_weights, self.cast(encoder_output,mstype.float32))\n","        output =  self.concat((embeddings, attn_applied))\n","        output = self.attn_combine(output)\n","\n","\n","        embeddings = self.trans(embeddings, self.perm)\n","        output, hidden = self.gru(embeddings, hidden)\n","        output = self.cast(output, mstype.float32)\n","        output = self.out(output)\n","        output = self.logsoftmax(output)\n","\n","        return output, hidden, attn_weights\n","\n","class Seq2Seq(nn.Cell):\n","    def __init__(self, config, is_train=True):\n","        super(Seq2Seq, self).__init__()\n","        self.max_len = config.max_seq_length\n","        self.is_train = is_train\n","\n","        self.encoder = Encoder(config, is_train)\n","        self.decoder = Decoder(config, is_train)\n","        self.expanddims = P.ExpandDims()\n","        self.squeeze = P.Squeeze(axis=0)\n","        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n","        self.concat = P.Concat(axis=1)\n","        self.concat2 = P.Concat(axis=0)\n","        self.select = P.Select()\n","\n","    def construct(self, src, dst):\n","        encoder_output, hidden = self.encoder(src)\n","        decoder_hidden = self.squeeze(encoder_output[self.max_len-2:self.max_len-1:1, ::, ::])\n","        if self.is_train:\n","            outputs, _ = self.decoder(dst, decoder_hidden, encoder_output)\n","        else:\n","            decoder_input = dst[::,0:1:1]\n","            decoder_outputs = ()\n","            for i in range(0, self.max_len):\n","                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n","                                                                 decoder_hidden, encoder_output)\n","                decoder_hidden = self.squeeze(decoder_hidden)\n","                decoder_output, _ = self.argmax(decoder_output)\n","                decoder_output = self.squeeze(decoder_output)\n","                decoder_outputs += (decoder_output,)\n","                decoder_input = decoder_output\n","            outputs = self.concat(decoder_outputs)\n","        return outputs\n","\n","class NLLLoss(_Loss):\n","    '''\n","       NLLLoss function\n","    '''\n","    def __init__(self, reduction='mean'):\n","        super(NLLLoss, self).__init__(reduction)\n","        self.one_hot = P.OneHot()\n","        self.reduce_sum = P.ReduceSum()\n","\n","    def construct(self, logits, label):\n","        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n","                                     F.scalar_to_array(0.0))\n","        #print('NLLLoss label_one_hot:',label_one_hot, label_one_hot.shape)\n","        #print('NLLLoss logits:',logits, logits.shape)\n","        #print('xxx:', logits * label_one_hot)\n","        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n","        return self.get_loss(loss)\n","    \n","class WithLossCell(nn.Cell):\n","    def __init__(self, backbone, config):\n","        super(WithLossCell, self).__init__(auto_prefix=False)\n","        self._backbone = backbone\n","        self.batch_size = config.batch_size\n","        self.onehot = nn.OneHot(depth=config.ch_vocab_size)\n","        self._loss_fn = NLLLoss()\n","        self.max_len = config.max_seq_length\n","        self.squeeze = P.Squeeze()\n","        self.cast = P.Cast()\n","        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n","        self.print = P.Print()\n","\n","    def construct(self, src, dst, label):\n","        out = self._backbone(src, dst)\n","        loss_total = 0\n","        for i in range(self.batch_size):\n","            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n","                                 self.squeeze(label[i:i+1:1, ::]))\n","            loss_total += loss\n","        loss = loss_total / self.batch_size\n","        return loss"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[],"source":["network = Seq2Seq(cfg)\n","network = WithLossCell(network, cfg)\n","optimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\n","model = Model(network, optimizer=optimizer)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4 模型训练"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 1 step: 125, loss is 2.838659\n","epoch time: 77082.398 ms, per step time: 616.659 ms\n","epoch: 2 step: 125, loss is 2.5093772\n","epoch time: 11390.392 ms, per step time: 91.123 ms\n","epoch: 3 step: 125, loss is 1.8508363\n","epoch time: 11374.811 ms, per step time: 90.998 ms\n","epoch: 4 step: 125, loss is 1.8408301\n","epoch time: 11262.442 ms, per step time: 90.100 ms\n","epoch: 5 step: 125, loss is 1.3256171\n","epoch time: 11227.342 ms, per step time: 89.819 ms\n","epoch: 6 step: 125, loss is 1.0894792\n","epoch time: 11234.876 ms, per step time: 89.879 ms\n","epoch: 7 step: 125, loss is 0.80114794\n","epoch time: 11338.926 ms, per step time: 90.711 ms\n","epoch: 8 step: 125, loss is 0.7878199\n","epoch time: 11508.560 ms, per step time: 92.068 ms\n","epoch: 9 step: 125, loss is 0.35237563\n","epoch time: 11505.715 ms, per step time: 92.046 ms\n","epoch: 10 step: 125, loss is 0.31488234\n","epoch time: 11493.019 ms, per step time: 91.944 ms\n","epoch: 11 step: 125, loss is 0.19495572\n","epoch time: 11536.087 ms, per step time: 92.289 ms\n","epoch: 12 step: 125, loss is 0.11501935\n","epoch time: 11556.475 ms, per step time: 92.452 ms\n","epoch: 13 step: 125, loss is 0.13036935\n","epoch time: 11534.854 ms, per step time: 92.279 ms\n","epoch: 14 step: 125, loss is 0.20907374\n","epoch time: 11541.235 ms, per step time: 92.330 ms\n","epoch: 15 step: 125, loss is 0.069087505\n","epoch time: 11546.282 ms, per step time: 92.370 ms\n"]}],"source":["loss_cb = LossMonitor()\n","config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps, keep_checkpoint_max=cfg.keep_checkpoint_max)\n","ckpoint_cb = ModelCheckpoint(prefix=\"gru\", directory=cfg.ckpt_save_path, config=config_ck)\n","time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n","callbacks = [time_cb, ckpoint_cb, loss_cb]\n","\n","model.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=True)"]},{"cell_type":"markdown","metadata":{},"source":["## 4.推理部分"]},{"cell_type":"code","execution_count":73,"metadata":{"trusted":true},"outputs":[],"source":["class InferCell(nn.Cell):\n","    def __init__(self, network, config):\n","        super(InferCell, self).__init__(auto_prefix=False)\n","        self.expanddims = P.ExpandDims()\n","        self.network = network\n","\n","    def construct(self, src, dst):\n","        out = self.network(src, dst)\n","        return out"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[],"source":["network = Seq2Seq(cfg,is_train=False)\n","network = InferCell(network, cfg)\n","network.set_train(False)\n","parameter_dict = load_checkpoint(cfg.checkpoint_path)\n","load_param_into_net(network, parameter_dict)\n","model = Model(network)"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[],"source":["#打开词汇表\n","with open(os.path.join(cfg.dataset_path,\"en_vocab.txt\"), 'r', encoding='utf-8') as f:\n","    data = f.read()\n","en_vocab = list(data.split('\\n'))\n","\n","with open(os.path.join(cfg.dataset_path,\"ch_vocab.txt\"), 'r', encoding='utf-8') as f:\n","    data = f.read()\n","ch_vocab = list(data.split('\\n'))\n","\n","def translate(str_en):\n","    max_seq_len = 10\n","    str_vocab = normalizeString(str_en).split(' ')\n","    print(\"English\",str(str_vocab))\n","    str_id = [[]]\n","    for i in str_vocab:\n","        str_id[0] += [en_vocab.index(i)]\n","   \n","    num = max_seq_len - len(str_id[0])\n","    if(num >= 0):\n","        str_id[0] += [0]*num\n","    else:\n","        str_id[0] = str_id[:max_seq_len] + [0]\n","    str_id = Tensor(np.array(str_id).astype(np.int32))\n","   \n","    out_id = Tensor(np.array([[1,0]]).astype(np.int32))\n","    \n","    output = network(str_id, out_id)\n","    out= ''\n","    for x in output[0].asnumpy():\n","        if x == 0:\n","            break\n","        out += ch_vocab[x]\n","    print(\"中文\",out)"]},{"cell_type":"code","execution_count":75,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["English ['i', 'love', 'tom']\n","中文 我爱汤姆。\n"]}],"source":["translate('i love tom')"]}],"metadata":{"kernelspec":{"display_name":"Mindspore-python3.7-aarch64","language":"python","name":"mindspore-python3.7-aarch64"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
